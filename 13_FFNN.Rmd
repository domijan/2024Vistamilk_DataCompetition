---
title: "FFNN"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




```{r load_libraries, warning = FALSE, message=FALSE}
library(tidyverse)
library(ggthemes)
library(gridExtra)
# https://tensorflow.rstudio.com/tutorials/keras/regression

# if (tensorflow::tf$executing_eagerly())
#   tensorflow::tf$compat$v1$disable_eager_execution()
# 
# library(keras)
# # K <- keras::backend()

library(reticulate)
use_condaenv("tf-2.6") 
tensorflow <- import("tensorflow")
# Libraries ---------------------------------------------------------------
library(keras)
```

```{r read_data}

calcRMSE <- function(y, yhat){sqrt(mean((yhat - y)^2))}


specTrain <- readRDS("specTrain.rds")
specTest <- readRDS("specTest.rds")

# train <- read.csv("data/training.csv")
dim(specTrain)
# names(train)

sample <- specTrain$`sample number` |> unique()
set.seed(1951)
random_split <- replicate(1, sample(length(sample), 40))
```


<!-- Convert to absorbance -->
<!-- ```{r abs} -->
<!-- x_tr <- log10(1/x_tr) -->
<!-- ``` -->




## Split and scale data

```{r split_scale}

# lapply(dat.tr, function(x) sum(is.na(x))) |> str()

dat.tr <- specTrain |> 
  filter(`sample number` %in% sample[random_split[,1]])
dat.te <- specTrain |> 
  filter(`sample number` %in% sample[-random_split[,1]])


train_data <- dat.tr|> select(c(`412.71`: `3907.24`))
train_type <- dat.tr |> select(`lactose content` )
train_type <- as.matrix(train_type )

test_data <- dat.te|> select(c(`412.71`: `3907.24`))
test_type <- dat.te |> select(`lactose content` )
test_type <- as.matrix(test_type )

x_train <- array_reshape(as.matrix(train_data), c(nrow(train_data), ncol(train_data)), order = "F")

mn <- apply(x_train, 2, mean)
sd <- apply(x_train, 2, sd)

x_train <- sweep(x_train,2, mn, "-")
x_train <- sweep(x_train,2, sd, "/")


# 
# p_scaled <- x_train |>
#   as_tibble() |>
#   mutate(id = 1:nrow(x_train), train_type = as.factor(train_type)) |>
#   pivot_longer(V1:V540) |>
#   mutate(name = as.numeric(str_remove(name, "V"))) |>
#   ggplot(aes(x= name, y = value, group = id,  col = train_type)) +
#   geom_line(aes(alpha = 0.02)) +
#   scale_colour_colorblind() +
#   guides(alpha="none", col = "none") +
#   ylab("AU") +
#   xlab(expression("Wavenumber" ~ cm^-1)) +
#   ggtitle("Scaled Training Data")
# 


x_test <- array_reshape(as.matrix(test_data), c(nrow(test_data),ncol(test_data)), order = "F")



x_test <- sweep(x_test,2, mn, "-")
x_test <- sweep(x_test,2, sd, "/")


# p_scaled_test <- x_test |>
#   as_tibble() |>
#   mutate(id = 1:nrow(x_test), train_type = as.factor(train_type)) |>
#   pivot_longer(V1:V540) |>
#   mutate(name = as.numeric(str_remove(name, "V"))) |>
#   ggplot(aes(x= name, y = value, group = id,  col = train_type)) +
#   geom_line(aes(alpha = 0.02)) +
#   scale_colour_colorblind() +
#   guides(alpha="none", col = "none") +
#   ylab("AU") +
#   xlab(expression("Wavenumber" ~ cm^-1)) +
#   ggtitle("Scaled Testing Data")

# 540 vs 70
```

```{r eval = FALSE}
grid.arrange(p_scaled, p_scaled_test,  nrow = 2)
```


## FFNN


```{r}



input_shape <- c(1813)



cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')


# Parameters --------------------------------------------------------------



batch_size <- 128
epochs <-  40# 40
```


```{r eval  = FALSE}


# Model defn --------------------------------------------------------------
normalizer <- layer_normalization(input_shape = shape(dim(x_train)[[2]]), axis = NULL)

# normalizer %>% adapt(x_train)


model <- keras_model_sequential() 

# DIDNT CONVERGE
 # model |> normalizer() |>
 #  layer_dense(1024, activation = 'elu') |> 
 #  layer_dropout(rate = 0.2) |>
 #  layer_dense(units = 512, activation = 'elu') |> 
 #  layer_dropout(rate = 0.2) |> 
 #    layer_dense(units = 256, activation = 'elu') |> 
 #  layer_dropout(rate = 0.2) |> 
 #    layer_dense(units = 128, activation = 'elu') |> 
 #  layer_dropout(rate = 0.2) |> 
 #   layer_dense(units = 64, activation = 'elu') |> 
 #  layer_dropout(rate = 0.2) |> 
 #     layer_dense(units = 32, activation = 'elu') |> 
 #  layer_dropout(rate = 0.2) |> 
 #  layer_dense(units = 1)

model |> normalizer() |>
    layer_dense(units = 64,
              activation = "relu",
              kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01)) |>
  layer_dense(units = 64,
              activation = "relu",
              kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01)) |>
  layer_dense(units = 64,
              activation = "relu",
              kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01))|>
  layer_dense(units = 64,
              activation = "relu",
              kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01))|>
  layer_dense(units = 1)




# OLD
  # layer_dense(units = 64,
  #             activation = "relu",
  #             kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01),
  #             input_shape = dim(x_train)[[2]]) |>
  # layer_dense(units = 64,
  #             activation = "relu",
  #             kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01)) |>
  # layer_dense(units = 1)
  # 
  




summary(model)
# Compile model ----------------------------------------------------

# model |> compile(
  # optimizer = optimizer_adam(learning_rate = 0.1),
  # loss = 'mean_absolute_error'
# )
model |> compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_sgd(learning_rate = 0.01, decay=1e-6, momentum = 0.9, nesterov = TRUE)
)

# Fit model to data
history <- model |> fit(
  x_train, train_type,
  batch_size = batch_size,
  epochs = 300,
  verbose = 1,
  validation_split = 0.2
)


```


```{r}

plot(history)

score <- model |> evaluate(
  x_test, test_type,
  verbose = 0
)

# Output metrics
cat('Test loss:', score[[1]], '\n')


pr <- predict(model, x_test)
calcRMSE(test_type, pr)
plot(test_type, pr)
```

